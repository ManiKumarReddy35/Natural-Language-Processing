# -*- coding: utf-8 -*-
"""stance_detection_logistic_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hHrlNcyZR5XGYVpDYN414cWBcdhlGAR_

### Importing the libraries
"""

import pandas as pd
import numpy as np

pd.set_option('display.max_colwidth', -1)

import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

from nltk import word_tokenize
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))

from nltk.stem import PorterStemmer
ps = PorterStemmer()

from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score

import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings("ignore")

"""#### Upload the files"""

train = pd.read_csv('train.csv',engine = 'python')
test = pd.read_csv('test.csv',engine= 'python')

train = train.sample(frac=1).reset_index(drop=True)
test = test.sample(frac=1).reset_index(drop=True) # donald trump

# removing all the rows of Xtest that contains donald trump

test = test[test['Target'] != 'Donald Trump']
test.shape

"""### Visualization of data"""

train.head()

"""### Cleaning the data"""

Xtrain = train[['Tweet','Target']]
ytrain = train['Stance']

Xtest = test[['Tweet','Target']]
ytest = test['Stance']

print('Traing data features shape ',Xtrain.shape)
print('Test data features shape',Xtest.shape)

print('Train data target shape ',ytrain.shape)
print('Test data target shape ',ytest.shape)

def category_encoder(data):
    samp = data['Target']
    samp[samp == 'Hillary Clinton'] = 'hillary'
    samp[samp == 'Feminist Movement'] = 'feminism'
    samp[samp == 'Legalization of Abortion'] = 'abortion'
    samp[samp == 'Atheism'] = 'atheism'
    samp[samp == 'Climate Change is a Real Concern'] = 'climate'

    dummy = pd.get_dummies(data['Target'])
    data = pd.concat([data,dummy],axis = 1)
    del data['Target']

    return data

def label_encoder(samp):
    samp[samp == 'AGAINST'] = 0
    samp[samp == 'FAVOR'] = 1
    samp[samp == 'NONE'] = 2

    return samp

def preprocess(text):
  mod_words = []
  words = text.split()
  for word in words:
      if word[0] == '@':
          continue
      if word[0] == '#':
          word = word[1:]

      word = word.lower()
      symbols = ['.',',','?','!']
      if len(word) == 0:
          continue

      if word[-1] in symbols:
          word = word[:-1]
      if len(word) > 2 and word not in stop_words and word.isalpha():
        word = lemmatizer.lemmatize(word)
        mod_words.append(word)

  text = ''
  for word in mod_words:
    text = text + ' '+ word
  return text

Xtrain = category_encoder(Xtrain)
Xtest = category_encoder(Xtest)

ytrain = label_encoder(ytrain)
ytest = label_encoder(ytest)

ytrain = ytrain.astype('int')
ytest = ytest.astype('int')

print('Traing data features shape ',Xtrain.shape)
print('Test data features shape',Xtest.shape)

print('Train data target shape ',ytrain.shape)
print('Test data target shape ',ytest.shape)

Xtrain['Tweet'] = Xtrain['Tweet'].apply(preprocess)
Xtest['Tweet'] = Xtest['Tweet'].apply(preprocess)

"""#### After Pre processing :"""

print('---- Train Data ----')
print(Xtrain.head())
print('---- Test Data ----')
print(Xtest.head())

"""### 3. Model

#### 3.1 Word Embeddings using TF-IDF
"""

tweet_array = vectorizer.fit_transform(Xtrain['Tweet']).toarray()
rem_data = np.array(Xtrain[['abortion','atheism','climate','feminism','hillary']])
Xtrain = np.hstack((tweet_array,rem_data)) 

tweet_array = vectorizer.transform(Xtest['Tweet']).toarray()
rem_data = np.array(Xtest[['abortion','atheism','climate','feminism','hillary']])
Xtest = np.hstack((tweet_array,rem_data))

print('Training data shape :',Xtrain.shape)
print('Test data shape:',Xtest.shape)

"""##### pickling the data"""

import pickle
out = open('Xtrain.pkl','wb')
pickle.dump(Xtrain,out)


out = open('Xtest.pkl','wb')
pickle.dump(Xtest,out)


out = open('Ytrain.pkl','wb')
pickle.dump(ytrain,out)


out = open('ytest.pkl','wb')
pickle.dump(ytest,out)

"""#### 3.1.1 Logistic Regression with tfidf"""

logreg = LogisticRegression(penalty = 'l2',solver='liblinear',multi_class='ovr',max_iter=1000)
logistic_classifier = logreg.fit(Xtrain,ytrain)

ypred_train  = logistic_classifier.predict(Xtrain)
ypred_test  = logistic_classifier.predict(Xtest)

print('F1 score for train data :',f1_score(ytrain, ypred_train, average='macro'))
print('F1 score for test data :',f1_score(ytest, ypred_test, average='macro'))

"""#### Grid Search CV"""

penalty = ['l1', 'l2']

C = np.logspace(-4, 4, 20)
print(C)
hyperparameters = dict(C=C, penalty=penalty)

from sklearn.model_selection import GridSearchCV
clf = GridSearchCV(LogisticRegression(), hyperparameters, cv=5, verbose=0)
best_model = clf.fit(Xtrain, ytrain)

print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])
print('Best C:', best_model.best_estimator_.get_params()['C'])

ypred_train  = best_model.predict(Xtrain)
ypred_test  = best_model.predict(Xtest)

print('F1 score for train data :',f1_score(ytrain, ypred_train, average='macro'))
print('F1 score for test data :',f1_score(ytest, ypred_test, average='macro'))

"""#### 3.1.2 Visualization of results

##### 3.1.3.1  Accuracies
"""

cls = ['class 0','class 1','class 2']
train_acc = f1_score(ytrain, ypred_train, average=None)
plt.bar(cls,train_acc)
plt.title('Training Accuracies')
plt.xlabel('Classes')
plt.ylabel('Accuracies')
plt.show()

test_acc = f1_score(ytest, ypred_test, average=None)
plt.bar(cls,test_acc)
plt.title('Test Accuracies')
plt.xlabel('Classes')
plt.ylabel('Accuracies')
plt.show()

"""##### 3.1.3.2 Classification matrix"""

import seaborn as sns
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(ytest,ypred_test)
df_cm = pd.DataFrame(cm,index = [i for i in cls],columns = [i for i in cls])
plt.figure()
sns.heatmap(df_cm,annot = True)
plt.title('Confusion Matrix')
plt.show()

"""##### 3.1.3.3 ROC Curve"""

pip install scikit-plot

import scikitplot as skp

prob = logistic_classifier.predict_proba(Xtest)
skp.metrics.plot_roc_curve(ytest,prob,title = 'ROC curve TF-IDF with Logistic regression',curves = 'each_class')



